import streamlit as st
import nibabel as nib
import numpy as np
from scipy import ndimage
from skimage.measure import label, regionprops
import matplotlib.pyplot as plt
import tempfile
import os
import subprocess
from pathlib import Path
import shutil

# PyTorch ì„í¬íŠ¸ (ì„ íƒì )
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    st.warning("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="CT Eye Detection & Alignment",
    page_icon="ğŸ‘ï¸",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'temp_dir' not in st.session_state:
    st.session_state.temp_dir = None

def create_temp_directory():
    """ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    temp_dir = tempfile.mkdtemp()
    return temp_dir

def cleanup_temp_directory(temp_dir):
    """ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
    if temp_dir and os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)

def load_model_and_predict(ct_path, model_path):
    """ì§ì ‘ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ë¡  ìˆ˜í–‰"""
    if not TORCH_AVAILABLE:
        st.error("PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return None, None, None
        
    import torch
    try:
        from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor
    except ImportError:
        st.error("nnUNetv2ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return None, None, None
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    st.info(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    try:
        # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=device)
        
        # nnUNet predictor ì´ˆê¸°í™”
        predictor = nnUNetPredictor(
            tile_step_size=0.5,
            use_gaussian=True,
            use_mirroring=True,
            perform_everything_on_gpu=True if device.type == 'cuda' else False,
            device=device,
            verbose=False,
            verbose_preprocessing=False,
            allow_tqdm=True
        )
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        predictor.initialize_from_trained_model_folder(
            model_folder_path=os.path.dirname(model_path),
            use_folds=None,
            checkpoint_name=os.path.basename(model_path)
        )
        
        # CT ë°ì´í„° ë¡œë“œ
        ct_nii = nib.load(ct_path)
        ct_data = ct_nii.get_fdata()
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (nnUNet í˜•ì‹)
        ct_data_input = ct_data[np.newaxis, ...]  # ì±„ë„ ì°¨ì› ì¶”ê°€
        
        # ì¶”ë¡  ìˆ˜í–‰
        with torch.no_grad():
            prediction = predictor.predict_from_data_iterator(
                data_iterator=[(ct_data_input, {'spacing': ct_nii.header.get_zooms()})],
                save_probabilities=False,
                num_processes_segmentation_export=1
            )
        
        # ê²°ê³¼ ì¶”ì¶œ
        mask_data = prediction[0][1]  # segmentation mask
        
        return ct_data, mask_data, ct_nii
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, None, None


def simple_inference_with_torch(ct_path, model_path):
    """ë‹¨ìˆœí™”ëœ PyTorch ì§ì ‘ ì¶”ë¡ """
    if not TORCH_AVAILABLE:
        st.error("PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return None, None, None
        
    import torch
    import torch.nn.functional as F
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # CT ë°ì´í„° ë¡œë“œ
        ct_nii = nib.load(ct_path)
        ct_data = ct_nii.get_fdata()
        
        # ì •ê·œí™” (nnUNet ìŠ¤íƒ€ì¼)
        ct_data = np.clip(ct_data, -1024, 3071)  # HU ë²”ìœ„ ì œí•œ
        mean = np.mean(ct_data)
        std = np.std(ct_data)
        ct_data_norm = (ct_data - mean) / (std + 1e-8)
        
        # í…ì„œë¡œ ë³€í™˜
        ct_tensor = torch.from_numpy(ct_data_norm).float()
        ct_tensor = ct_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, D, H, W)
        ct_tensor = ct_tensor.to(device)
        
        # ëª¨ë¸ ë¡œë“œ
        model = torch.load(model_path, map_location=device)
        if isinstance(model, dict):
            # state_dictì¸ ê²½ìš°
            st.warning("ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. nnUNet ëª¨ë¸ í´ë” ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
            return None, None, None
        
        model.eval()
        
        # ì¶”ë¡ 
        with torch.no_grad():
            output = model(ct_tensor)
            
            # Softmax ì ìš© (ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš°)
            if output.shape[1] > 1:
                output = F.softmax(output, dim=1)
                mask = torch.argmax(output, dim=1)
            else:
                mask = torch.sigmoid(output) > 0.5
        
        # numpyë¡œ ë³€í™˜
        mask_data = mask.squeeze().cpu().numpy()
        
        return ct_data, mask_data, ct_nii
        
    except Exception as e:
        st.error(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, None, None

def find_eye_centers(mask_data):
    """ë§ˆìŠ¤í¬ì—ì„œ ëˆˆ ì¤‘ì‹¬ì  ì°¾ê¸°"""
    labeled_3d = label(mask_data > 0)
    regions_3d = regionprops(labeled_3d)
    
    centers = []
    for region in regions_3d:
        z_center, y_center, x_center = region.centroid
        centers.append((x_center, y_center))
    
    centers_sorted = sorted(centers, key=lambda c: c[0])
    return centers_sorted, len(regions_3d)

def calculate_rotation_angle(centers):
    """ë‘ ì¤‘ì‹¬ì ì˜ ê¸°ìš¸ê¸°ë¡œ íšŒì „ ê°ë„ ê³„ì‚°"""
    if len(centers) < 2:
        return 0, "ë‹¨ì¼ ì˜ì—­ë§Œ ê²€ì¶œë¨"
    
    left_eye = centers[0]
    right_eye = centers[1]
    
    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    
    angle_rad = np.arctan2(dy, dx)
    angle_deg = np.degrees(angle_rad)
    
    return -angle_deg, None

def rotate_volume(volume_data, angle_deg):
    """ë³¼ë¥¨ ë°ì´í„° íšŒì „"""
    rotated = ndimage.rotate(
        volume_data,
        angle_deg,
        axes=(1, 2),
        reshape=False,
        order=1,
        mode='constant',
        cval=volume_data.min()
    )
    return rotated

def create_visualization(ct_data, mask_data, rotated_ct, rotated_mask, angle):
    """ì‹œê°í™” ìƒì„±"""
    mask_slices = np.where(mask_data.sum(axis=(1,2)) > 0)[0]
    
    if len(mask_slices) == 0:
        return None
    
    # ì¤‘ê°„ ìŠ¬ë¼ì´ìŠ¤ ì„ íƒ
    center_slice = mask_slices[len(mask_slices)//2]
    
    fig, axes = plt.subplots

# create_visualization í•¨ìˆ˜ ê³„ì†
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    # ì›ë³¸ CT
    axes[0].imshow(ct_data[center_slice, :, :], cmap='gray')
    axes[0].set_title('Original CT')
    axes[0].axis('off')
    
    # ì›ë³¸ CT + Mask
    axes[1].imshow(ct_data[center_slice, :, :], cmap='gray')
    axes[1].imshow(mask_data[center_slice, :, :], cmap='Reds', alpha=0.5)
    axes[1].set_title('Original + Mask')
    axes[1].axis('off')
    
    # íšŒì „ëœ CT
    axes[2].imshow(rotated_ct[center_slice, :, :], cmap='gray')
    axes[2].set_title(f'Rotated CT ({angle:.1f}Â°)')
    axes[2].axis('off')
    
    # íšŒì „ëœ CT + Mask
    axes[3].imshow(rotated_ct[center_slice, :, :], cmap='gray')
    axes[3].imshow(rotated_mask[center_slice, :, :], cmap='Reds', alpha=0.5)
    axes[3].set_title('Rotated + Mask')
    axes[3].axis('off')
    
    plt.tight_layout()
    return fig

# Streamlit UI
st.title("ğŸ§  CT Eye Detection & Alignment Pipeline")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    inference_method = st.radio(
        "ì¶”ë¡  ë°©ë²•",
        ["PyTorch ëª¨ë¸ íŒŒì¼ (.pth)", "nnUNet ëª¨ë¸ í´ë”"],
        index=0
    )
    
    if inference_method == "PyTorch ëª¨ë¸ íŒŒì¼ (.pth)":
        model_file = st.file_uploader(
            "ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ",
            type=['pth', 'pt'],
            help="í•™ìŠµëœ best_model.pth íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        if model_file:
            # ëª¨ë¸ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_file:
                tmp_file.write(model_file.getbuffer())
                model_path = tmp_file.name
                st.session_state.model_path = model_path
                st.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_file.name}")
    else:
        model_path = st.text_input(
            "nnUNet ëª¨ë¸ í´ë” ê²½ë¡œ",
            value="nnUNet_results/Dataset001_Eyeball/nnUNetTrainer__nnUNetPlans__3d_fullres",
            help="í•™ìŠµëœ nnUNet ëª¨ë¸ì´ ìˆëŠ” í´ë” ê²½ë¡œ"
        )
        st.session_state.model_path = model_path
    
    st.markdown("---")
    
    # ì¶”ë¡  ì„¤ì •
    st.subheader("ğŸ”§ ì¶”ë¡  ì„¤ì •")
    
    use_gpu = st.checkbox("GPU ì‚¬ìš©", value=TORCH_AVAILABLE and torch.cuda.is_available() if TORCH_AVAILABLE else False)
    
    if use_gpu and TORCH_AVAILABLE and not torch.cuda.is_available():
        st.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    elif not TORCH_AVAILABLE:
        st.error("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    batch_size = st.number_input("ë°°ì¹˜ í¬ê¸°", min_value=1, max_value=8, value=1)
    
    st.markdown("---")
    st.markdown("### ğŸ“ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    1. ëª¨ë¸ íŒŒì¼(.pth) ë˜ëŠ” ê²½ë¡œ ì„¤ì •
    2. CT íŒŒì¼(.nii.gz) ì—…ë¡œë“œ
    3. 'Process' ë²„íŠ¼ í´ë¦­
    4. ê²°ê³¼ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
    """)
    
    # PyTorch ì„í¬íŠ¸ í™•ì¸
    if TORCH_AVAILABLE:
        st.sidebar.success(f"PyTorch {torch.__version__} ì‚¬ìš© ê°€ëŠ¥")
        if torch.cuda.is_available():
            st.sidebar.info(f"GPU: {torch.cuda.get_device_name(0)}")
    else:
        st.sidebar.error("PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        st.sidebar.info("ì„¤ì¹˜: pip install torch torchvision")

# ë©”ì¸ ì»¨í…ì¸ 
col1, col2 = st.columns([1, 2])

with col1:
    st.header("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "CT íŒŒì¼ ì„ íƒ (.nii.gz)",
        type=['nii.gz'],
        help="NIfTI í˜•ì‹ì˜ CT ìŠ¤ìº” íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    if uploaded_file is not None:
        st.success(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")
        
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        file_size = uploaded_file.size / (1024 * 1024)  # MB
        st.info(f"íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

with col2:
    st.header("ğŸ”„ ì²˜ë¦¬ ìƒíƒœ")
    
    if uploaded_file is not None:
        if st.button("ğŸš€ Process", type="primary", use_container_width=True):
            st.session_state.processed = False
            
            # ì§„í–‰ ìƒíƒœ í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                if st.session_state.temp_dir:
                    cleanup_temp_directory(st.session_state.temp_dir)
                
                temp_dir = create_temp_directory()
                st.session_state.temp_dir = temp_dir
                
                # 1. íŒŒì¼ ì €ì¥
                status_text.text("ğŸ“ íŒŒì¼ ì €ì¥ ì¤‘...")
                progress_bar.progress(10)
                
                input_dir = os.path.join(temp_dir, "input")
                os.makedirs(input_dir, exist_ok=True)
                
                input_path = os.path.join(input_dir, uploaded_file.name)
                with open(input_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # 2. ëª¨ë¸ ì¶”ë¡ 
                status_text.text("ğŸ§  AI ëª¨ë¸ë¡œ ëˆˆ ê²€ì¶œ ì¤‘...")
                progress_bar.progress(30)
                
                # ëª¨ë¸ì´ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                if 'model_path' not in st.session_state:
                    st.error("âŒ ëª¨ë¸ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
                    progress_bar.empty()
                    status_text.empty()
                    st.stop()
                
                # ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ 
                if inference_method == "PyTorch ëª¨ë¸ íŒŒì¼ (.pth)":
                    # ê°„ë‹¨í•œ ì¶”ë¡  (ëª¨ë¸ êµ¬ì¡°ë¥¼ ëª¨ë¥´ëŠ” ê²½ìš°)
                    ct_data, mask_data, ct_nii = simple_inference_with_torch(
                        input_path, 
                        st.session_state.model_path
                    )
                else:
                    # nnUNet ìŠ¤íƒ€ì¼ ì¶”ë¡ 
                    ct_data, mask_data, ct_nii = load_model_and_predict(
                        input_path,
                        st.session_state.model_path
                    )
                
                # ì¶”ë¡  ì‹¤íŒ¨ ì‹œ ë°ëª¨ ë°ì´í„° ì‚¬ìš©
                if ct_data is None or mask_data is None:
                    st.warning("âš ï¸ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨. ë°ëª¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    
                    ct_nii = nib.load(input_path)
                    ct_data = ct_nii.get_fdata()
                    
                    # ê°€ìƒì˜ ë§ˆìŠ¤í¬ ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶œë ¥ ì‚¬ìš©)
                    mask_data = np.zeros_like(ct_data)
                    center_z = ct_data.shape[0] // 2
                    for x_offset in [-60, 60]:
                        for z in range(max(0, center_z-5), min(ct_data.shape[0], center_z+5)):
                            for y in range(200, 280):
                                for x in range(256+x_offset-30, 256+x_offset+30):
                                    if ((y-240)**2 + (x-256-x_offset)**2) < 900:
                                        mask_data[z, y, x] = 1
                
                progress_bar.progress(60)
                
                # 3. ì¤‘ì‹¬ì  ì°¾ê¸° ë° ê°ë„ ê³„ì‚°
                status_text.text("ğŸ“ ì •ë ¬ ê°ë„ ê³„ì‚° ì¤‘...")
                progress_bar.progress(70)
                
                centers, num_regions = find_eye_centers(mask_data)
                angle, error_msg = calculate_rotation_angle(centers)
                
                if error_msg:
                    st.warning(f"âš ï¸ {error_msg}")
                    angle = 0
                
                # 4. íšŒì „ ìˆ˜í–‰
                status_text.text("ğŸ”„ ì´ë¯¸ì§€ íšŒì „ ì¤‘...")
                progress_bar.progress(80)
                
                if abs(angle) > 0.5:
                    rotated_ct = rotate_volume(ct_data, angle)
                    rotated_mask = rotate_volume(mask_data, angle)
                else:
                    rotated_ct = ct_data
                    rotated_mask = mask_data
                    st.info("íšŒì „ ê°ë„ê°€ 0.5ë„ ë¯¸ë§Œì´ì–´ì„œ íšŒì „í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                # 5. ê²°ê³¼ ì €ì¥
                status_text.text("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
                progress_bar.progress(90)
                
                # ê²°ê³¼ íŒŒì¼ ì €ì¥
                result_ct_path = os.path.join(temp_dir, "aligned_ct.nii.gz")
                result_mask_path = os.path.join(temp_dir, "aligned_mask.nii.gz")
                
                rotated_ct_nii = nib.Nifti1Image(rotated_ct, ct_nii.affine, ct_nii.header)
                rotated_mask_nii = nib.Nifti1Image(rotated_mask, ct_nii.affine, ct_nii.header)
                
                nib.save(rotated_ct_nii, result_ct_path)
                nib.save(rotated_mask_nii, result_mask_path)
                
                # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.processed = True
                st.session_state.results = {
                    'ct_data': ct_data,
                    'mask_data': mask_data,
                    'rotated_ct': rotated_ct,
                    'rotated_mask': rotated_mask,
                    'angle': angle,
                    'num_regions': num_regions,
                    'centers': centers,
                    'result_ct_path': result_ct_path,
                    'result_mask_path': result_mask_path
                }
                
                progress_bar.progress(100)
                status_text.text("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
                
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                progress_bar.empty()
                status_text.empty()
# ê²°ê³¼ í‘œì‹œ
if st.session_state.processed and 'results' in st.session_state:
    st.markdown("---")
    st.header("ğŸ“Š ê²°ê³¼")
    
    results = st.session_state.results
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê²€ì¶œëœ ì˜ì—­ ìˆ˜", results['num_regions'])
    with col2:
        st.metric("íšŒì „ ê°ë„", f"{results['angle']:.2f}Â°")
    with col3:
        st.metric("ì¤‘ì‹¬ì  ìˆ˜", len(results['centers']))
    
    # ì‹œê°í™”
    st.subheader("ğŸ–¼ï¸ ì‹œê°í™”")
    fig = create_visualization(
        results['ct_data'],
        results['mask_data'],
        results['rotated_ct'],
        results['rotated_mask'],
        results['angle']
    )
    
    if fig:
        st.pyplot(fig)
        plt.close()
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.subheader("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
    col1, col2 = st.columns(2)
    
    with col1:
        with open(results['result_ct_path'], 'rb') as f:
            st.download_button(
                label="ğŸ“¥ ì •ë ¬ëœ CT ë‹¤ìš´ë¡œë“œ",
                data=f.read(),
                file_name="aligned_ct.nii.gz",
                mime="application/gzip"
            )
    
    with col2:
        with open(results['result_mask_path'], 'rb') as f:
            st.download_button(
                label="ğŸ“¥ ì •ë ¬ëœ ë§ˆìŠ¤í¬ ë‹¤ìš´ë¡œë“œ",
                data=f.read(),
                file_name="aligned_mask.nii.gz",
                mime="application/gzip"
            )

# ì •ë¦¬
if st.button("ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬"):
    if st.session_state.temp_dir:
        cleanup_temp_directory(st.session_state.temp_dir)
        st.session_state.temp_dir = None
        st.session_state.processed = False
        st.success("âœ… ì„ì‹œ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center'>
        <p>CT Eye Detection & Alignment Pipeline v1.0</p>
    </div>
    """,
    unsafe_allow_html=True
)